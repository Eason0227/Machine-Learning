{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "from math import sqrt\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['label'] = iris.target\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "def TransformToList(data): #把資料轉成 list 呈現\n",
    "    data_list = list()\n",
    "    for i in range(len(data)):\n",
    "        data_list.append(list(data.iloc[i]))\n",
    "    return data_list\n",
    "\n",
    "train_data, test_data = train_test_split(df, random_state=123, train_size=0.8)\n",
    "\n",
    "dataset = TransformToList(df)\n",
    "train_list = TransformToList(train_data) \n",
    "test_list = TransformToList(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a dataset based on an attribute and an attribute value\n",
    "def data_split(index, value, dataset):\n",
    "\tleft, right = [],[]\n",
    "\tfor row in dataset: #讀取資料集中的每筆資料，判斷這在個特徵下的每筆資料屬於左邊還右邊\n",
    "\t\tif row[index] < value:\n",
    "\t\t\tleft.append(row)\n",
    "\t\telse:\n",
    "\t\t\tright.append(row)\n",
    "\treturn left, right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_index(groups, classes):\n",
    "\t# count all samples at split point\n",
    "\tn_instances = float(sum([len(group) for group in groups]))\n",
    "\t# sum weighted Gini index for each group\n",
    "\tgini = 0.0\n",
    "\tfor group in groups: #左子樹與右子樹\n",
    "\t\tsize = float(len(group))\n",
    "\t\tif size == 0: # avoid divide by zero\n",
    "\t\t\tcontinue\n",
    "\t\tscore = 0.0\n",
    "\t\tfor class_val in classes: \t# score the group based on the score for each class\n",
    "\t\t\tp = [row[-1] for row in group].count(class_val) / size\n",
    "\t\t\tscore += p * p\n",
    "\t\tgini += (1.0 - score) * (size / n_instances) # 根據左子樹與右子樹樣本的比例加權gini index\n",
    "\treturn gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(dataset, n_features):\n",
    "\tclass_values = list(set(row[-1] for row in dataset))\n",
    "\tbest_index, best_value, best_gini, best_groups = 999, 999, 999, None\n",
    "\tfeatures = list()\n",
    "\twhile len(features) < n_features: #隨機選擇n個特徵\n",
    "\t\tindex = randrange(len(dataset[0])-1) #隨機選一個特徵\n",
    "\t\tif index not in features:\n",
    "\t\t\tfeatures.append(index)\n",
    "\tfor index in features: \t#歷遍所有特徵的所有值\n",
    "\t\tfor row in dataset: #每筆資料\n",
    "\t\t\tgroups = data_split(index, row[index], dataset) #分割資料\n",
    "\t\t\tgini = gini_index(groups, class_values) #分割後的gini\n",
    "\t\t\tif gini < best_gini:\n",
    "\t\t\t\tbest_index = index\n",
    "\t\t\t\tbest_value = row[index]\n",
    "\t\t\t\tbest_gini = gini\n",
    "\t\t\t\tbest_groups = groups\n",
    "\treturn {'index':best_index, 'value':best_value, 'groups':best_groups}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a terminal node value\n",
    "def to_terminal(group):\n",
    "\toutcomes = [row[-1] for row in group]\n",
    "\treturn max(set(outcomes), key=outcomes.count)\n",
    " \n",
    "# Create child splits for a node or make terminal\n",
    "def split(node, max_depth, min_size, n_features, depth):\n",
    "\tleft, right = node['groups']\n",
    "\tdel(node['groups'])\n",
    "\tif not left or not right: \t# check for a no split \n",
    "\t\tnode['left'] = node['right'] = to_terminal(left + right) #無法分割時回傳結果\n",
    "\t\treturn\n",
    "\tif depth >= max_depth:\t# check for max depth\n",
    "\t\tnode['left'], node['right'] = to_terminal(left), to_terminal(right) #達最大深度時回傳結果\n",
    "\t\treturn\n",
    "\n",
    "\tif len(left) <= min_size:\t# process left child\n",
    "\t\tnode['left'] = to_terminal(left)\n",
    "\telse: #左子樹超過最小樣本，繼續分割\n",
    "\t\tnode['left'] = get_split(left, n_features)\n",
    "\t\tsplit(node['left'], max_depth, min_size, n_features, depth+1) #每次深度depth都會增加\n",
    "\n",
    "\tif len(right) <= min_size:\t# process right child\n",
    "\t\tnode['right'] = to_terminal(right)\n",
    "\telse: #右子樹超過最小樣本，繼續分割\n",
    "\t\tnode['right'] = get_split(right, n_features)\n",
    "\t\tsplit(node['right'], max_depth, min_size, n_features, depth+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a decision tree\n",
    "def decision_tree(train, max_depth, min_size, n_features):\n",
    "\troot = get_split(train, n_features)\n",
    "\tsplit(root, max_depth, min_size, n_features, 1)\n",
    "\treturn root\n",
    " \n",
    "# Make a prediction with a decision tree\n",
    "def predict(tree, row):\n",
    "\tif row[tree['index']] < tree['value']: # 預測值小於某個特徵的某個值，放到左子樹\n",
    "\t\tif isinstance(tree['left'], dict): # 檢查左子樹的型態是否是dictionary\n",
    "\t\t\treturn predict(tree['left'], row)\n",
    "\t\telse:\n",
    "\t\t\treturn tree['left']\n",
    "\telse:\n",
    "\t\tif isinstance(tree['right'], dict):\n",
    "\t\t\treturn predict(tree['right'], row)\n",
    "\t\telse:\n",
    "\t\t\treturn tree['right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random subsample from the dataset with replacement\n",
    "def subsample(dataset, ratio):\n",
    "    sample = list()\n",
    "    n_sample = round(len(dataset) * ratio)\n",
    "    while len(sample) < n_sample:\n",
    "        index = randrange(len(dataset))\n",
    "        sample.append(dataset[index])\n",
    "    return sample\n",
    " \n",
    "# Make a prediction with a list of bagged trees\n",
    "def bagging_predict(trees, row):\n",
    "    predictions = [predict(tree, row) for tree in trees]\n",
    "    return max(set(predictions), key=predictions.count)\n",
    " \n",
    "# Random Forest Algorithm\n",
    "def random_forest(train, test, max_depth, min_size, sample_size, n_trees, n_features):\n",
    "    trees = []\n",
    "    for i in range(n_trees):\n",
    "        sample = subsample(train, sample_size)\n",
    "        tree = decision_tree(sample, max_depth, min_size, n_features)\n",
    "        trees.append(tree)\n",
    "    predictions = [bagging_predict(trees, row) for row in test]\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "\tcorrect = 0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tif actual[i] == predicted[i]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn correct / float(len(actual)) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = df.drop(['label'],axis=1)\n",
    "Y = df['label']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size = 0.8, random_state=123)\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.66666666666667"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#n_folds = 5\n",
    "max_depth = 10\n",
    "min_size = 1\n",
    "sample_size = 0.8\n",
    "n_trees =100\n",
    "n_features = int(sqrt(len(dataset[0])-1))\n",
    "\n",
    "#tree = decision_tree(train_list, max_depth, min_size, n_features)\n",
    "#pred = [ predict(tree,row) for row in test_list]\n",
    "#accuracy_metric(actual,pred)\n",
    "\n",
    "pred = random_forest(train_list,test_list, max_depth, min_size, sample_size, n_trees , n_features)\n",
    "accuracy_metric(Y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.66666666666667"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(max_depth = 10 ,min_samples_leaf=1, min_samples_split=2 ,max_features='sqrt',n_estimators= 100)\n",
    "rf.fit(X_train,Y_train)\n",
    "test_pred = rf.predict(X_test)\n",
    "accuracy_metric(Y_test,test_pred)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1db13ed87bb8965e4f3a2d3f13e8dfe37cf8727ab6c757d84791f40d0f5929ff"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
